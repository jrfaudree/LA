\documentclass[11pt,fleqn]{article} 
\usepackage[margin=0.8in, head=0.8in]{geometry} 
\usepackage{amsmath, amssymb, amsthm}
\usepackage{fancyhdr} 
\usepackage{palatino, url, multicol}
\usepackage{graphicx,tabularx,systeme} 
\usepackage[all]{xy}
\usepackage{polynom} 
\usepackage{pdfsync}
\usepackage{enumerate}
\usepackage{framed}
\usepackage{setspace}
\usepackage{array,tikz}

\newcommand{\bbm}{\begin{bmatrix}}
\newcommand{\ebm}{\end{bmatrix}}

\newcommand{\Reals}{\mathbb{R}}
\def\vectwo#1#2{\begin{bmatrix}#1\\#2\end{bmatrix}}
\def\vecthree#1#2#3{\begin{bmatrix}#1\\#2\\#3\end{bmatrix}}
\def\vecfour#1#2#3#4{\begin{bmatrix}#1\\#2\\#3\\#4\end{bmatrix}}

%\pagestyle{fancy} 
\lfoot{Linear}
\rfoot{FE study}
\renewcommand{\familydefault}{\sfdefault}



\begin{document}
\begin{center}{\Large{Final Exam Questions}}\end{center}

		\begin{enumerate}
		\item How to compute $\Vert v \Vert$ and what does it mean?
		\item How do you find the angle between two vectors? How do you know if two vectors are orthogonal (perpendicular) or at an acute (or obtuse) angle.
		\item Assume you are given a collection of vectors, say $v_1, v_2, v_3$, how can you \emph{show} they are linearly independent? Linearly dependent? 
		\item Assume you are given a collection of vectors, say $v_1, v_2, v_3$, how can you show that they form a basis? That they do not form a basis? That they are an orthonormal basis?
		\item Assume a set of vectors is a basis, say $v_1, v_2, v_3$, and you are given another vector, say $v$, how do you write $v$ vector as a linear combination of $v_1, v_2, v_3$? What are the advantages of an orthonormal set of vectors?
		\item Given vectors $a_1$, $a_2$, $a_3$, how do you implement the Gram-Schmidt algorithm?  What properties do the $q$'s have? How are those properties related to the $a$'s?
		\item If $q_1$, $q_2$ and $q_3$ are orthonormal in $\Reals^3$, explain how you know they form a basis?  Since they do, given
$x\in\Reals^3$ we can write $x = \alpha_1 q_1+\alpha_2 q_2+ \alpha_3 q_3$
for some numbers $\alpha_1$, $\alpha_2$ and $\alpha_3$.  What are the numbers
$\alpha_k$?  Hint: they can be expressed using inner products: equation (5.5) in the text.	
	\item Can you find the matrix $A$ such that $Ax$  is the transformation of the plane that rotates $x$ by 60 degrees and then reflects it about the $y$ axis?
	\item Given a vector $a$ and a vector $b$, how do you compute the convolution $a*b$?
What is the matrix $T$ such that $a*b = Tb$?
\item Know how to represent a linear function in terms of a matrix.\\  
For example, suppose $f$ is the function that takes $(x_1, x_2, x_3)$ to $((x_2+x_3)/2,-x_3, x_1)$.
What is its representation in terms of a matrix?  
\item Similarly, suppose 
$f$ is a linear map from $R^2$ to $R^4$ and $f(e_1) = (-1, 3, 4, 3)$ and $f(e_2) = (2 , 3, 4, 9)$.
What is the representation of $f$ in terms of a matrix?
\item How can you show that a function is linear? Affine? Not affine? Think up your own examples of functions from $R^3$ to $R^3$ that are linear, affine, and neither.
\item If asked for the coefficients of a quadratic polynomial $p$ with $p(x_i)=y_i$ for $i=1,\ldots, 5$,
can you set up a system of linear equations to solve for the coefficients?
\item How do you solve $Ax=b$ if $A$ is lower triangular?  What if $A$ is upper triangular?
	\item How do you check that a matrix $A$ is orthogonal?
	\item How do you check that the \emph{columns} of the matrix $A$ form an orthogonal set of vectors? How is this question different from the previous one? 
	\item What is the column perspective of matrix-matrix multiplication?  What is the row perspective?
	\item Express the following task as a matrix algebra task: "Find a linear combination of vectors $a_1$, $a_2$ and $a_3$
that equals $b$".  This gets at the column interpretation of matrix-vector multiplication (page 119).
	\item What is the transpose of a matrix product?
	\item If you multiply a $3\times 3$ matrix $A$ on the left by $\mathrm{diag}(1,2,3)$ what
is the result?  How about if you multiply $A$ on the right 
by $\mathrm{diag}(1,2,3)$?
	\item Find a $4\times 5$ matrix $L$ such that when you multiply any $5\times k$ matrix
$A$ on the left by $L$, the result is the matrix $A$ with its bottom row removed.
	\item Given a matrix $A$ with linearly independent columns,
how do you compute its QR factorization?  This is related to the Gram-Schmidt algorithm,
and you should review how you convert the steps of the Gram-Schmidt algorithm into
the entries of the matrices of the QR factorization. \textbf{You will be asked to show 
that you know how to do this.}  See also homework 8,
additional problem 2.
	\item Now, given the QR factorization of a square matrix $A$, how do you solve $Ax=b$?
This is a two step procedure.  If I give you $Q$ and $R$, can you carry out the steps?
	\item Suppose $A$ has a left inverse.  Show that the columns of $A$ have to be linearly independent.
	\item Once you know that a square matrix has linearly independent columns you know a whole bunch 
of other things are true.  Name as many as you can.  How many solutions of $Ax=0$ are there?  Why?
	\item Similarly, once you know that a square matrix is \textbf{not} invertible (or is singular), you know a whole bunch of things. Name as many as you can. How many solutions to $Ax=0$ are there?
	\item What is the inverse of a matrix product of invertible matrices? What is the inverse of the transpose of an invertible matrix?	
	\item Suppose $A$ is an $3\times 3$ matrix.  How do you find $A^{-1}$? 
If you have a QR factorization of $A$, how do you find $A^{-1}$?
	\item How to you find the pseudoinverse of a matrix $A$?
	\item How can you use the pseudoinverse of a matrix $A$?
	\item How do you find  \emph{the least squares approximate solution} of $Ax=b,$ namely $\hat{x},$ and how do your interpret it?
	\item What are the \emph{normal equations} are for a system $Ax=b.$
	\item If $A$ has a $QR$-factorization, what is it's pseudoinverse?
	\item Suppose $A$ is tall and $A=QR,$ where $Q$ has orthonormal columns and $R$ is upper-triangular, How do your compute the pseudo inverse $A^\dagger$? How do you find the least squares approximate solution to $Ax=b$?
	\item Translate a problem -- such as approximating a given set of points with a polynomial -- to a least squares approximation problem. Set up a matrix system to solve this problem and solve it. (See HW 10 \#3.)
	\item Suppose $\hat{x}$ is the least squares approximate solution to $Ax=b$ and suppose $y$ is any other $n$-vector. What can you conclude about $\hat{x}$ and $y$?
	\item You should know what the null space of a matrix $A$ is, how to find it, and how it relates to linear functions and matrix invertibility.
	\item Suppose you have two vectors $w_1=(1,2,3,4)$ and $w_2=(2,3,4,5),$. Set up a matrix $A$ such that the null space of $A$ is the set of all vectors perpendicular to both $w_1$ and $w_2.$ Then determine the set of vectors in this null space.
	\item Suppose $A$ is a wide matrix and $w$ is a solution to $Ax=b,$ how do you find \emph{all} solutions to $Ax=b$?
	\item How do your find the determinant of a matrix, including very large but sparse ones. 
	\item You should know basic "determinant algebra" including
		\begin{itemize}
		\item how elementary row operations and matrix operation ($+$, $*$, inverses) affect the determinant
		\item how the determinant relates to the null space and, thus, invertibility
		\end{itemize}
	\item You should know what eigenvalues and eigenvectors of a matrix $A$ are, how to find them, how to confirm them, and what they indicate about $A$ as a linear function.
	\item What are the eigenvalues of an upper triangular matrix? 
	\item Given a square matrix $A$ and vector $v$, how would you check if $v$ was an eigenvector of $A$? 
	\item If $x$ is an eigenvector of $A$ with eigenvalue $\lambda$, what can you say about $A^kx$ for a positive integer $k$?
	\end{enumerate}

\end{document}
